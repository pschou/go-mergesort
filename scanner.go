// Merge sort will sort a set of io.Reader by fields in parallel and return the
// result.  Similar to how bufio.Scanner works, it is up to the reader
// interface to make sure that memory is copied or used per scan call as
// mergesort reuses the byte slices upon a new scan call.
package mergesort

import (
	"bufio"
	"bytes"
	"context"
	"errors"
	"io"
	"sync"
)

// This scanner interface is similar to the bufio.Scanner interface so as to
// make implementation in already existing code trivial.  The input type has to
// be a io.Reader interface so as to avoid any collisions in the parallel
// sorting reusing the same bytes buffer.  Effort has been made to avoid any
// memory copies.
type Scanner struct {
	rdrs    []io.Reader // The readers provided by the client.
	split   SplitFunc   // The function to split the tokens.
	compare CompareFunc // The function to compare the tokens for sorting or deduplication.
	filter  FilterFunc  // The function to filter lines for formatting after splitting.

	cur        []byte        // Slice pointer to the current element.
	c          chan (*penny) // Channel from the top level of sorting.
	err        error         // Sticky error.
	scanCalled bool          // Scan has been called; buffer is in use.
	done       bool          // Scan finnished.

	ctx    context.Context
	cancel func()
}

// Unit of work, this is needed as the buffer needs to be returned to the pool
// once the read boundary has been crossed.
type penny struct {
	dat      []byte     // Pointer to data which is next in this sort pool.
	datReady sync.Mutex // If filtering is used, indicate that the data is ready for use.

	id   int    // Index of file data is from.
	err  error  // Pass up any errors.
	toDo func() // Function todo upon completion.
}

// Pool to use for reading in the underlying data
var pool sync.Pool = sync.Pool{
	New: func() any {
		return make([]byte, 32*1024)
	},
}

// Text returns the most recent token generated by a call to Scanner.Scan as a
// newly allocated string holding bytes.
func (s *Scanner) Text() string {
	return string(s.cur)
}

// Bytes returns the most recent token generated by a call to Scanner.Scan. The
// underlying array may point to data that will be overwritten by a subsequent
// call to Scan.
func (s *Scanner) Bytes() []byte {
	return s.cur
}

// Err returns the first non-EOF error that was encountered by the [Scanner].
func (s *Scanner) Err() error {
	if s.err == io.EOF {
		return nil
	}
	return s.err
}

// Scan advances the Scanner to the next token, which will then be available
// through the Scanner.Bytes or Scanner.Text method. It returns false when
// there are no more tokens, either by reaching the end of the input or an
// error.
func (s *Scanner) Scan() bool {
	if s.done {
		return false
	}
	if !s.scanCalled {
		s.scanCalled = true
		if len(s.rdrs) == 0 {
			s.done = true
			return false
		}

		// The sorters do the bulk of the work (in parallel).  The ideal scenaio of
		// worker routines is a triangle number where if there are N inputs N(N-1)/2
		// sorters are allocated.
		idx := make([]int, len(s.rdrs))
		for i := range idx {
			idx[i] = i
		}
		s.c = make(chan (*penny), 10)
		makeSorters(s.ctx, s.c, s.split, s.compare, s.filter, s.rdrs, idx, s.cancel)
	}

	ctr := 0
	for {
		if ctr++; ctr > 1000 {
			s.cancel()
			for cleanup := range s.c {
				if cleanup.toDo != nil {
					cleanup.toDo()
				}
			}
			s.done = true
			panic("Too many reads without any data")
		}
		t, more := <-s.c
		s.done = !more
		if t.toDo != nil {
			t.toDo()
			continue
		}
		if t.err != nil {
			// Error was encountered, terminate read
			s.err, s.done = t.err, true
			s.cancel()
			for cleanup := range s.c {
				if cleanup.toDo != nil {
					cleanup.toDo()
				}
			}
			return false
		}
		s.cur = t.dat
		return true
	}
}

func read(ctx context.Context, c chan (*penny), r io.Reader, id int, split SplitFunc) {
	var (
		n     int // read size
		total int // total size read
		err   error
		atEOF bool

		adv      int
		tok      []byte
		splitErr error
	)
	buf := pool.Get()
	b := buf.([]byte)
	defer func() {
		c <- &penny{toDo: func() {
			// Return previous memory buffer to pool
			pool.Put(buf)
		}}
		if err != nil {
			c <- &penny{err: err}
		}
		close(c)
	}()

	//data_walk:
	for {
		select {
		case <-ctx.Done():
			return // returning so as to not leak the goroutine
		default:
			// Add data to the buffer
			n, err = r.Read(b[total:])
			if err != nil {
				atEOF = err == io.EOF
				if !atEOF {
					return
				}
			}
			total = total + n

			// Slice the input buffer into tokens and send them to the chan
			for total > 0 {
				adv, tok, splitErr = split(b[:total], atEOF, id)
				if adv > total {
					err = bufio.ErrAdvanceTooFar
					return
				} else if adv < 0 {
					err = bufio.ErrNegativeAdvance
					return
				}
				if splitErr == bufio.ErrFinalToken {
					if len(tok) > 0 {
						c <- &penny{dat: tok, id: id}
					}
					err = io.EOF
					return
				}
				if err != nil {
					err = splitErr
					return
				}
				// Return the token if a value is returned
				if len(tok) > 0 {
					c <- &penny{dat: tok, id: id}
				}
				// Trim down the slice to what remains and loop
				b = b[adv:]
				total = total - adv
				if adv == 0 { // Nothing consumed
					// If nothing was returned and we are on the last loop
					if total > 0 && atEOF {
						c <- &penny{dat: b[:total], id: id}
					}
					break
				}
			}

			// If an error is encountered, return the error and close channel
			if err != nil {
				return
			}

			if total == len(b) {
				// If the buffer is filled, create a new buffer, copy the slice and
				// flush the used buffer.
				next := pool.Get()
				nb := next.([]byte)
				copy(nb, b)
				c <- &penny{toDo: func() {
					// Return previous memory buffer to pool
					pool.Put(buf)
				}}
				buf, b = next, nb
			}
		}
	}
}

var flushMe = errors.New("Flush me")

func makeSorters(ctx context.Context, c chan (*penny), split SplitFunc, comp CompareFunc, filt FilterFunc, list []io.Reader, idx []int, cancel func()) {
	if len(list) == 1 {
		// Handle the case when one reader is left
		if filt == nil {
			// No filtering is needed, so just read from the disk
			go read(ctx, c, list[0], idx[0], split)
		} else {
			// When filtering is needed, make a channel and queue up a loop to do filtering
			reader := make(chan (*penny), 10)
			sender := make(chan (*penny), 10)
			go read(ctx, reader, list[0], idx[0], split)

			// Range over the reader to populate the sender
			go func() {
				defer close(sender)
				for dat := range reader {
					if dat.err != nil {
						sender <- dat
					} else if dat.toDo != nil {
						dat.err = flushMe
						sender <- dat
					} else {
						dat.datReady.Lock()
						go func(dat *penny) {
							dat.dat, dat.toDo, dat.err = filt(dat.dat, idx[0])
							dat.datReady.Unlock()
						}(dat)
						sender <- dat
					}
				}
			}()

			// Range over the sender to populate the channel for sorting
			go func() {
				defer close(c)
				for dat := range sender {
					if dat.err != nil {
						if dat.err == flushMe {
							// Special case when reader provided the toDo action
							dat.err = nil
						}
						c <- dat
						continue
					}
					dat.datReady.Lock()
					defer dat.datReady.Unlock()
					if len(dat.dat) == 0 && dat.toDo == nil {
						// Drop empty records!
						continue
					}
					if dat.toDo != nil {
						// Split out records with actions
						c <- &penny{dat: dat.dat}
						dat.dat = nil
					}
					c <- dat
				}
			}()
		}
		return
	}
	mid := len(list) / 2
	a := make(chan (*penny), 10)
	b := make(chan (*penny), 10)
	makeSorters(ctx, a, split, comp, filt, list[:mid], idx[:mid], nil)
	makeSorters(ctx, b, split, comp, filt, list[mid:], idx[mid:], nil)
	go func() {
		defer close(c)
		var (
			aDat, bDat *penny
			aMore      = true
			hasA       = false
			bMore      = true
			hasB       = false
		)
		for {
			select {
			case <-ctx.Done():
				for cleanup := range a {
					if cleanup.toDo != nil {
						cleanup.toDo()
					}
				}
				for cleanup := range b {
					if cleanup.toDo != nil {
						cleanup.toDo()
					}
				}
				return // returning so as to not leak the goroutine
			default:
				if !hasA && aMore {
					// Get next data element(s) for comparison
					aDat, aMore = <-a
					if aDat.toDo != nil || aDat.err != nil {
						if aDat.err == io.EOF {
							aMore = false
							hasA = false
						} else {
							c <- aDat // Send the toDo or err
						}
						continue
					} else if len(aDat.dat) > 0 {
						hasA = true
					}
				}
				if !hasB && bMore {
					bDat, bMore = <-b
					if bDat.toDo != nil || bDat.err != nil {
						if bDat.err == io.EOF {
							bMore = false
							hasB = false
						} else {
							c <- bDat // Send the toDo or error
						}
						continue
					} else if len(bDat.dat) > 0 {
						hasB = true
					}
				}
				if !hasA && !hasB {
					c <- &penny{err: io.EOF}
					if cancel != nil {
						cancel()
					}
					return
				}
				if hasA && !hasB {
					c <- aDat
					hasA = false
				} else if !hasA && hasB {
					c <- bDat
					hasB = false
				} else if hasA && hasB {
					x := comp(aDat.dat, bDat.dat, aDat.id, bDat.id)

					switch x {
					case -2: // A is wanted more, so it goes first and B is ignored
						c <- aDat
						hasA = false
						hasB = false
					case -1, 0: // A is less, so it goes first
						c <- aDat
						hasA = false
					case 1: // B is less, so it goes first
						c <- bDat
						hasB = false
					case 2: // B is wanted more, so it goes first and A is ignored
						c <- bDat
						hasA = false
						hasB = false
					}
				} else {
					return // Nothing more to do, should not get here
				}
			}
		}
	}()
}

// Compare returns an integer comparing two byte slices lexicographically. The
// result will be 0 if a == b, -1 if a < b, and +1 if a > b. A nil argument is
// equivalent to an empty slice.
//
// Record removal is also possible, if -2 is provided then only `a` will be
// used and if +2 is provided then only `b` will be used.
type CompareFunc func(a, b []byte, ai, bi int) int

// Filter allows one to filter results just after scanning.  If there is
// computational work needed to be done (like type conversions or looking up an
// index in a map) the filter is the place to do this, not in the SplitFunc.
//
// The used() func is called after a record has been used (if one is provided).
// The purpose of the used func handle is to help with memory management.  An
// example of the use case for used is to return a byte slice to a sync.Pool.
type FilterFunc func(input []byte, id int) (output []byte, used func(), err error)

// SplitFunc is the signature of the split function used to tokenize the input.
// The arguments are an initial substring of the remaining unprocessed data and
// a flag, atEOF, that reports whether the [Reader] has no more data to give,
// and i, which is the index of the [Reader] input. The return values are the
// number of bytes to advance the input and the next token to return to the
// user, if any, plus an error, if any.
//
// Scanning stops if the function returns an error, in which case some of the
// input may be discarded. If that error is [ErrFinalToken], scanning stops
// with no error. A non-nil token delivered with [ErrFinalToken] will be the
// last token, and a nil token with [ErrFinalToken] immediately stops the
// scanning.
//
// Otherwise, the [Scanner] advances the input. If the token is not nil, the
// [Scanner] returns it to the user. If the token is nil, the Scanner reads
// more data and continues scanning; if there is no more data--if atEOF was
// true--the [Scanner] returns. If the data does not yet hold a complete token,
// for instance if it has no newline while scanning lines, a [SplitFunc] can
// return (0, nil, nil) to signal the [Scanner] to read more data into the
// slice and try again with a longer slice starting at the same point in the
// input.
//
// The function is never called with an empty data slice unless atEOF is true.
// If atEOF is true, however, data may be non-empty and, as always, holds
// unprocessed text.
type SplitFunc func(data []byte, atEOF bool, i int) (advance int, token []byte, err error)

// Simple comparison function
func BytesCompare(a, b []byte, ai, bi int) int {
	return bytes.Compare(a, b)
}

// Simple comparison function with deduplication between files
func BytesCompareDedup(a, b []byte, ai, bi int) (c int) {
	c = bytes.Compare(a, b)
	if c == 0 {
		c = -2
	}
	return
}

// ScanLines is a split function for a [Scanner] that returns each line of
// text, stripped of any trailing end-of-line marker. The returned line may be
// empty. The end-of-line marker is one optional carriage return followed by
// one mandatory newline. In regular expression notation, it is `\r?\n`.  The
// last non-empty line of input will be returned even if it has no newline.
func ScanLines(data []byte, atEOF bool, i int) (advance int, token []byte, err error) {
	return bufio.ScanLines(data, atEOF)
}

// Cancel will close all the go routines and channels.
func (s *Scanner) Cancel() {
	s.cancel()
}

// NewScanner returns a new Scanner to read from a set of scanners which expect
// ordered input.
//
// As the bulk of the sorting is done in goroutines and in the background, the
// context is the best method to cancel any on-going sorting functions.
func New(ctx context.Context, list ...io.Reader) *Scanner {
	myCtx, cancel := context.WithCancel(ctx)
	return &Scanner{
		rdrs:    list,
		split:   ScanLines,
		compare: BytesCompare,
		ctx:     myCtx,
		cancel:  cancel,
	}
}

// Split sets the split function for the [Scanner].
// The default split function is [ScanLines].
//
// Split panics if it is called after scanning has started.
func (s *Scanner) Split(split SplitFunc) {
	if s.scanCalled {
		panic("Split called after Scan")
	}
	s.split = split
}

// Compare sets the compare function for the [Scanner].
// The default compare function is [BytesCompare].
//
// Compare panics if it is called after scanning has started.
func (s *Scanner) Compare(compare CompareFunc) {
	if s.scanCalled {
		panic("Compare called after Scan")
	}
	s.compare = compare
}

// Filter sets the filter function for the [Scanner].
// The default filter function is none.
//
// Filter panics if it is called after scanning has started.
func (s *Scanner) Filter(filter FilterFunc) {
	if s.scanCalled {
		panic("Filter called after Scan")
	}
	s.filter = filter
}
