// Merge sort will sort a set of io.Reader by fields in parallel and return the
// result.  Similar to how bufio.Scanner works, it is up to the reader
// interface to make sure that memory is copied or used per scan call as
// mergesort reuses the byte slices upon a new scan call.
package mergesort

import (
	"bufio"
	"bytes"
	"context"
	"io"
	"sync"
)

// This scanner interface is similar to the bufio.Scanner interface so as to
// make implementation in already existing code trivial.  The input type has to
// be a io.Reader interface so as to avoid any collisions in the parallel
// sorting reusing the same bytes buffer.  Effort has been made to avoid any
// memory copies.
type Scanner struct {
	// Slice pointer to the current element in memory
	cur []byte
	// Worker doing the top level of sorting
	c chan (penny)
	// Error pass up
	err error

	ctx    context.Context
	cancel func()
}

// Unit of work, this is needed as the buffer needs to be returned to the pool
// once the read boundary has been crossed.
type penny struct {
	// Pointer to data which is next in this sort pool.
	dat []byte
	id  int
	// Pass up of any errors
	err error
	// We crossed a boundary mark and the old memory pool can be released once we
	// are at this penny.
	toFlush any
}

// Pool to use for reading in the underlying data
var pool sync.Pool = sync.Pool{
	New: func() any {
		return make([]byte, 32*1024)
	},
}

// Text returns the most recent token generated by a call to Scanner.Scan as a
// newly allocated string holding bytes.
func (s *Scanner) Text() string {
	return string(s.cur)
}

// Bytes returns the most recent token generated by a call to Scanner.Scan. The
// underlying array may point to data that will be overwritten by a subsequent
// call to Scan.
func (s *Scanner) Bytes() []byte {
	return s.cur
}

// Scan advances the Scanner to the next token, which will then be available
// through the Scanner.Bytes or Scanner.Text method. It returns false when
// there are no more tokens, either by reaching the end of the input or an
// error.
func (s *Scanner) Scan() bool {
	if s.err != nil {
		// No more reads are possible
		return false
	}
	ctr := 0
	for {
		ctr++
		if ctr > 10 {
			return false
		}
		t, _ := <-s.c
		if t.toFlush != nil {
			// Return previous memory buffer to pool
			pool.Put(t.toFlush)
			continue
		}
		if t.err != nil {
			// Error was encountered, terminate read
			s.err = t.err
			s.cancel()
			return false
		}
		s.cur = t.dat
		return true
	}
}

func read(ctx context.Context, c chan (penny), r io.Reader, id int, split bufio.SplitFunc) {
	var (
		n     int // read size
		total int // total size read
		err   error
		atEOF bool

		adv      int
		tok      []byte
		splitErr error
	)
	buf := pool.Get()
	b := buf.([]byte)
	defer func() {
		c <- penny{toFlush: buf}
		c <- penny{err: err}
		close(c)
	}()

	//data_walk:
	for {
		select {
		case <-ctx.Done():
			return // returning so as to not leak the goroutine
		default:
			// Add data to the buffer
			n, err = r.Read(b[total:])
			atEOF = err == io.EOF
			total = total + n

			// Slice the input buffer into tokens and send them to the chan
			for total > 0 {
				adv, tok, splitErr = split(b[:total], atEOF)
				if adv > total {
					err = bufio.ErrAdvanceTooFar
					return
				} else if adv < 0 {
					err = bufio.ErrNegativeAdvance
					return
				}
				if err == bufio.ErrFinalToken {
					c <- penny{dat: tok, id: id}
					err = io.EOF
					return
				}
				if err != nil {
					err = splitErr
					return
				}
				if adv == 0 { // Nothing consumed
					break
				} else {
					// Trim down the slice to what remains, return token, and loop
					b = b[adv:]
					total = total - adv
					c <- penny{dat: tok, id: id}
				}
			}

			// If an error is encountered, return the error and close channel
			if err != nil {
				return
			}

			if total == len(b) {
				// If the buffer is filled, create a new buffer, copy the slice and
				// flush the old buffer.
				next := pool.Get()
				nb := next.([]byte)
				copy(nb, b)
				c <- penny{toFlush: buf}
				buf, b = next, nb
			}
		}
	}
}

func makeSorters(ctx context.Context, c chan (penny), split bufio.SplitFunc, comp CompareFunc, list []io.Reader, idx []int) {
	if len(list) == 1 {
		// Handle the case when one reader is given
		go read(ctx, c, list[0], idx[0], split)
		return
	}
	mid := len(list) / 2
	a := make(chan (penny), 2)
	b := make(chan (penny), 2)
	makeSorters(ctx, a, split, comp, list[:mid], idx[:mid])
	makeSorters(ctx, b, split, comp, list[mid:], idx[mid:])
	go func() {
		defer func() {
			for {
				// Flush all channels
				select {
				case <-a:
				case <-b:
				default:
					break
				}
			}
			// Close return channel
			close(c)
		}()
		var (
			aDat, bDat penny
			aMore      = true
			hasA       = false
			bMore      = true
			hasB       = false
		)
		for {
			select {
			case <-ctx.Done():
				return // returning so as to not leak the goroutine
			default:
				if !hasA && aMore {
					// Get next data element(s) for comparison
					aDat, aMore = <-a
					if aDat.toFlush != nil || aDat.err != nil {
						if aDat.err == io.EOF {
							aMore = false
							hasA = false
						} else {
							c <- aDat // Send the flush or err
						}
						continue
					} else {
						hasA = true
					}
				}
				if !hasB && bMore {
					bDat, bMore = <-b
					if bDat.toFlush != nil || bDat.err != nil {
						if bDat.err == io.EOF {
							bMore = false
							hasB = false
						} else {
							c <- bDat // Send the flush or error
						}
						continue
					} else {
						hasB = true
					}
				}
				if !hasA && !hasB {
					c <- penny{err: io.EOF}
					return
				}
				if hasA && !hasB {
					c <- aDat
					hasA = false
				} else if !hasA && hasB {
					c <- bDat
					hasB = false
				} else if hasA && hasB {
					x := comp(aDat.dat, bDat.dat, aDat.id, bDat.id)
					switch x {
					case -2: // A is wanted more, so it goes first and B is ignored
						c <- aDat
						hasA = false
						hasB = false
					case -1, 0: // A is less, so it goes first
						c <- aDat
						hasA = false
					case 1: // B is less, so it goes first
						c <- bDat
						hasB = false
					case 2: // B is wanted more, so it goes first and A is ignored
						c <- bDat
						hasA = false
						hasB = false
					}
				} else {
					return // Nothing more to do, should not get here
				}
			}
		}
	}()
}

// Compare returns an integer comparing two byte slices lexicographically. The
// result will be 0 if a == b, -1 if a < b, and +1 if a > b. A nil argument is
// equivalent to an empty slice.
//
// Filtering by record is also possible, if -2 is provided then only a will be
// used and if +2 is provided then only b will be used.
type CompareFunc func(a, b []byte, ai, bi int) int

// Simple comparison function
func BytesCompare(a, b []byte, ai, bi int) int {
	return bytes.Compare(a, b)
}

// Simple comparison function with deduplication between files
func BytesCompareDedup(a, b []byte, ai, bi int) (c int) {
	c = bytes.Compare(a, b)
	if c == 0 {
		c = -2
	}
	return
}

// NewScanner returns a new Scanner to read from a set of scanners which expect
// ordered input.
//
// As the bulk of the sorting is done in goroutines and in the background, the
// context is the best method to cancel any on-going sorting functions.
func New(ctx context.Context, split bufio.SplitFunc, comp CompareFunc, list ...io.Reader) *Scanner {
	c := make(chan (penny), 2)
	myCtx, cancel := context.WithCancel(ctx)
	// The sorters do the bulk of the work (in parallel).  The ideal scenaio of
	// worker routines is a triangle number where if there are N inputs N(N-1)/2
	// sorters are allocated.
	idx := make([]int, len(list))
	for i := range idx {
		idx[i] = i
	}
	makeSorters(ctx, c, split, comp, list, idx)
	return &Scanner{
		ctx:    myCtx,
		cancel: cancel,
		c:      c,
	}
}
